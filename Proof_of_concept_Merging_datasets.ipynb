{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNS_Research_work.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaKs0OD31-84"
      },
      "source": [
        "## Using Pyspark framework since it has inbuilt SQL functionality for data wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXbDn28iY_ye",
        "outputId": "ca61ca0d-adbe-4de2-d4fc-d1b136cd9d0b"
      },
      "source": [
        "!pip install pyspark\n",
        "import pyspark, re\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.types import StringType, StructField, StructType\n",
        "from os import listdir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAvFX714aUk2",
        "outputId": "b1505ad1-440e-4122-e395-60ed93733972"
      },
      "source": [
        "spark = SparkSession.builder.master('local[1]').appName('Azimuth_Launcher.com').getOrCreate()\n",
        "csvs = [fname for fname in listdir() if fname.endswith('csv')]\n",
        "print(csvs)\n",
        "print(spark)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lung.csv', 'pancreas.csv', 'kidney.csv', 'pbmc.csv', 'motor_cortex.csv']\n",
            "<pyspark.sql.session.SparkSession object at 0x7f67c0a15c10>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3uofw66kqSa"
      },
      "source": [
        "def preformat_union_csvs(file_name='kidney.csv', remove_top_8_lines=False):\n",
        "  \"\"\"\n",
        "        DOCSTRING:  Reads the raw csv of Azimuth data, optionally removes the top 8 lines and brings it in the required structure like the ASCT-B CCF reporter visualization tool.\n",
        "                    We will remove the first 8 lines, drop the *COUNT* columns, add a new column containing File-name for backtracking purposes and register a new Pyspark.SQL view.\n",
        "        INPUT:      csv file-name, resultant-df to append new dataframe to, flag to indicate removal of top 8 lines for merging.\n",
        "        OUTPUT:     Dataframe csv at target-directory, or error.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    df = spark.read.options(header=True, inferSchema=True).csv(file_name)\n",
        "\n",
        "    # Ignore the first 8 lines containing metadata\n",
        "    df = df.rdd.zipWithIndex().filter(lambda x: x[1] > 8).map(lambda x: x[0]).toDF()\n",
        "\n",
        "    # Rename the dataframe columns as the first row in current dataframe after deleting 8 lines\n",
        "    schema_from_first_row = StructType([ StructField(field_name, StringType(), True)  for field_name in df.first() ])\n",
        "    df = spark.createDataFrame(df.rdd, schema=schema_from_first_row)\n",
        "\n",
        "    # Drop the /COUNT/ columns for now\n",
        "    regex = re.compile(\".*COUNT.*\")\n",
        "    for col in list(filter(regex.match, df.schema.names)):\n",
        "      df = df.drop(col)\n",
        "    \n",
        "    # Add a FILE column for backtracking purposes\n",
        "    file_category = file_name.replace('.csv','')\n",
        "    df = df.withColumn('FILE',lit(file_category))\n",
        "    \n",
        "    # Create temp views in spark sql for later processing via SQL\n",
        "    df.createOrReplaceTempView(file_category)\n",
        "\n",
        "    print('\\nPre-formatted the input data for {} successfully!'.format(file_name))\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print('\\nSomething went wrong while pre-formatting the input data. Please check if the file is currently in use.')\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6_YaOZPj9cE",
        "outputId": "5683439e-85ab-44c0-fdf1-7b71d5f3dea2"
      },
      "source": [
        "anatomical_structure_dfs = {}\n",
        "# max_cols = float('-inf')\n",
        "# csv_with_max_cols = ''\n",
        "for csv in csvs:\n",
        "  anatomical_structure_dfs[csv] = preformat_union_csvs(csv)\n",
        "'''\n",
        "  if max_cols < len(anatomical_structure_dfs[csv].columns):\n",
        "    max_cols = len(anatomical_structure_dfs[csv].columns)\n",
        "    csv_with_max_cols = csv\n",
        "'''\n",
        "print(csv_with_max_cols,' : ',max_cols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pre-formatted the input data for lung.csv successfully!\n",
            "\n",
            "Pre-formatted the input data for pancreas.csv successfully!\n",
            "\n",
            "Pre-formatted the input data for kidney.csv successfully!\n",
            "\n",
            "Pre-formatted the input data for pbmc.csv successfully!\n",
            "\n",
            "Pre-formatted the input data for motor_cortex.csv successfully!\n",
            "motor_cortex.csv  :  13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haLIcIglpxbN"
      },
      "source": [
        "anatomical_structure_dfs['pbmc.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1-2' LAYER_CONNECTION, FILE from pbmc\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2-3' LAYER_CONNECTION, FILE from pbmc\n",
        "          \"\"\")\n",
        "anatomical_structure_dfs['pbmc.csv'].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0LjA_l1oUMQ"
      },
      "source": [
        "anatomical_structure_dfs['lung.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1-2' LAYER_CONNECTION, FILE from lung\n",
        "          \"\"\")\n",
        "anatomical_structure_dfs['lung.csv'].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-L3sxytoivp"
      },
      "source": [
        "anatomical_structure_dfs['pancreas.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`,  `AS/1` AS `AS/2`,  `AS/1/LABEL` as `AS/2/LABEL`, `AS/1/ID` as `AS/1/ID`, '1-2' LAYER_CONNECTION, FILE from pancreas\n",
        "          \"\"\")\n",
        "anatomical_structure_dfs['pancreas.csv'].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgwvj9rTpS3r"
      },
      "source": [
        "anatomical_structure_dfs['kidney.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1-2' LAYER_CONNECTION, FILE from kidney\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2-3' LAYER_CONNECTION, FILE from kidney\n",
        "          \"\"\")\n",
        "anatomical_structure_dfs['kidney.csv'].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00HCTV9Mjfr8"
      },
      "source": [
        "anatomical_structure_dfs['motor_cortex.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1-2' LAYER_CONNECTION, FILE from motor_cortex\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2-3' LAYER_CONNECTION, FILE from motor_cortex\n",
        "              UNION\n",
        "              SELECT `AS/3`, `AS/3/LABEL`, `AS/3/ID`, `AS/4`, `AS/4/LABEL`, `AS/4/ID`, '3-4' LAYER_CONNECTION, FILE from motor_cortex\n",
        "          \"\"\")\n",
        "anatomical_structure_dfs['motor_cortex.csv'].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfSBhTSBvOd2"
      },
      "source": [
        "## Further Preprocessing hardcoded for now, should be dynamic based on varying number of column-headers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4ahwkPxgCjv"
      },
      "source": [
        "# Manipulate each of the pyspark-sql views to bring them in the Source->Target network structure\n",
        "anatomical_structure_dfs['pbmc.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1->2' LAYER_CONNECTION, FILE from pbmc\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2->3' LAYER_CONNECTION, FILE from pbmc\n",
        "          \"\"\")\n",
        "\n",
        "anatomical_structure_dfs['lung.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1->2' LAYER_CONNECTION, FILE from lung\n",
        "          \"\"\")\n",
        "\n",
        "anatomical_structure_dfs['pancreas.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`,  `AS/1` AS `AS/2`,  `AS/1/LABEL` as `AS/2/LABEL`, `AS/1/ID` as `AS/1/ID`, '1->2' LAYER_CONNECTION, FILE from pancreas\n",
        "          \"\"\")\n",
        "\n",
        "anatomical_structure_dfs['kidney.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1->2' LAYER_CONNECTION, FILE from kidney\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2->3' LAYER_CONNECTION, FILE from kidney\n",
        "          \"\"\")\n",
        "\n",
        "anatomical_structure_dfs['motor_cortex.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1->2' LAYER_CONNECTION, FILE from motor_cortex\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2->3' LAYER_CONNECTION, FILE from motor_cortex\n",
        "              UNION\n",
        "              SELECT `AS/3`, `AS/3/LABEL`, `AS/3/ID`, `AS/4`, `AS/4/LABEL`, `AS/4/ID`, '3->4' LAYER_CONNECTION, FILE from motor_cortex\n",
        "          \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP28JlcMvgkd",
        "outputId": "35e4ada9-3db1-4bf1-e2b0-3a3eda304462"
      },
      "source": [
        "[(k, v.schema.names) for k,v in anatomical_structure_dfs.items()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lung.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/2/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE']),\n",
              " ('pancreas.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE']),\n",
              " ('kidney.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/2/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE']),\n",
              " ('pbmc.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/2/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE']),\n",
              " ('motor_cortex.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/2/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE'])]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtLYFFXPuqWK"
      },
      "source": [
        "## Was planning on making the SQL statements more dynamic to handle varying number of column-names.\n",
        "\n",
        "#### For now have hardcoded the logic for each dataframe Union/Merge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjp3VZhtM6HO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb55d7a-164f-49ee-a974-927923211fc8"
      },
      "source": [
        "# Create a dummy target dataframe to store a Source->Target network structure\n",
        "tgt_schema = StructType([\n",
        "                         StructField('AS/1',StringType(),True), StructField('AS/1/LABEL',StringType(),True), \n",
        "                         StructField('AS/1/ID',StringType(),True), StructField('AS/2',StringType(),True), \n",
        "                         StructField('AS/2/LABEL',StringType(),True), StructField('AS/2/ID',StringType(),True), \n",
        "                         StructField('LAYER_CONNECTION',StringType(),True), StructField('FILE',StringType(),True)\n",
        "                         ])\n",
        "tgt_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema = tgt_schema)\n",
        "\n",
        "\n",
        "# For each standardized dataset, union it with the dummy tgt dataframe we created\n",
        "for csv in csvs:\n",
        "  tgt_df = tgt_df.union(anatomical_structure_dfs[csv])\n",
        "  '''\n",
        "  if len(anatomical_structure_dfs[csv].schema.names) > 6:\n",
        "    anatomical_structure_dfs[csv] = make_pivot_unions(anatomical_structure_dfs[csv])\n",
        "  else:\n",
        "    anatomical_structure_dfs[csv] = impute_values(anatomical_structure_dfs[csv])\n",
        "  '''\n",
        "\n",
        "tgt_df.schema.names = ['LABEL', 'EXPANDED_LABEL', 'AS/1/ID', 'LABEL', 'EXPANDED_LABEL', 'AS/2/ID', 'LAYER_CONNECTION', 'FILE']\n",
        "tgt_df.summary().show()\n",
        "\n",
        "tgt_df.toPandas().to_csv('New_Consolidated_Network_Python.csv', index=False, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------+-----------------+--------------------+-------+----------+--------------------+----------------+------+\n",
            "|summary|          AS/1|       AS/1/LABEL|             AS/1/ID|   AS/2|AS/2/LABEL|             AS/2/ID|LAYER_CONNECTION|  FILE|\n",
            "+-------+--------------+-----------------+--------------------+-------+----------+--------------------+----------------+------+\n",
            "|  count|           517|              517|                 515|    515|       515|                 513|             518|   518|\n",
            "|   mean|          null|           2146.0|                null|   null|      null|                null|            null|  null|\n",
            "| stddev|          null|538.8153672641492|                null|   null|      null|                null|            null|  null|\n",
            "|    min|\",\"CL:1001107\"|             1765|             AS/1/ID|   AS/1|AS/1/LABEL|             AS/1/ID|            1->2|kidney|\n",
            "|    25%|          null|           1765.0|                null|   null|      null|                null|            null|  null|\n",
            "|    50%|          null|           1765.0|                null|   null|      null|                null|            null|  null|\n",
            "|    75%|          null|           2527.0|                null|   null|      null|                null|            null|  null|\n",
            "|    max|       schwann|             vein|[kidney loop of H...|schwann|      vein|[kidney loop of H...|            3->4|  pbmc|\n",
            "+-------+--------------+-----------------+--------------------+-------+----------+--------------------+----------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlmTDYdmsdxJ"
      },
      "source": [
        "### Was planning on creating a network with multiple layers in between, so would have been working with dataframes of different numbers of columns.\n",
        "```\n",
        "from pyspark.sql import SparkSession, HiveContext\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql import Row\n",
        "\n",
        "def custom_union(df1, df2):\n",
        "    df1_cols = df1.columns\n",
        "    df2_cols = df2.columns\n",
        "    total_cols = sorted(df1_cols + list(set(df2_cols) - set(df1_cols)))\n",
        "    def expr(mycols, allcols):\n",
        "        def process_cols(colname):\n",
        "            if colname in mycols:\n",
        "                return colname\n",
        "            else:\n",
        "                return lit(None).alias(colname)\n",
        "        cols = map(process_cols, allcols)\n",
        "        return list(cols)\n",
        "        \n",
        "    appended = df1.select(expr(df1_cols, total_cols)).union(df2.select(expr(df2_cols, total_cols)))\n",
        "    return appended\n",
        "\n",
        "\n",
        "df = custom_union(parent_df, anatomical_structure_dfs['pancreas.csv'])\n",
        "df = custom_union(df, anatomical_structure_dfs[''])\n",
        "```\n"
      ]
    }
  ]
}