{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "CNS_Research_work.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "3bb337e48977d9901755c563f6410b685f88fa579304c3c27b15121351fa38a8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Using Pyspark framework since it has inbuilt SQL functionality for data wrangling"
      ],
      "metadata": {
        "id": "qaKs0OD31-84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "!pip install pyspark\r\n",
        "import pyspark, re\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import lit\r\n",
        "from pyspark.sql.types import StringType, StructField, StructType\r\n",
        "from os import listdir"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXbDn28iY_ye",
        "outputId": "ca61ca0d-adbe-4de2-d4fc-d1b136cd9d0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "spark = SparkSession.builder.master('local[1]').appName('Azimuth_Launcher.com').getOrCreate()\r\n",
        "csvs = [fname for fname in listdir() if fname.endswith('csv')]\r\n",
        "print(csvs)\r\n",
        "print(spark)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1216/316711140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcsvs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAvFX714aUk2",
        "outputId": "b1505ad1-440e-4122-e395-60ed93733972"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "def preformat_union_csvs(file_name='kidney.csv', remove_top_8_lines=False):\r\n",
        "  \"\"\"\r\n",
        "        DOCSTRING:  Reads the raw csv of Azimuth data, optionally removes the top 8 lines and brings it in the required structure like the ASCT-B CCF reporter visualization tool.\r\n",
        "                    We will remove the first 8 lines, drop the *COUNT* columns, add a new column containing File-name for backtracking purposes and register a new Pyspark.SQL view.\r\n",
        "        INPUT:      csv file-name, resultant-df to append new dataframe to, flag to indicate removal of top 8 lines for merging.\r\n",
        "        OUTPUT:     Dataframe csv at target-directory, or error.\r\n",
        "  \"\"\"\r\n",
        "  try:\r\n",
        "    df = spark.read.options(header=True, inferSchema=True).csv(file_name)\r\n",
        "\r\n",
        "    # Ignore the first 8 lines containing metadata\r\n",
        "    df = df.rdd.zipWithIndex().filter(lambda x: x[1] > 8).map(lambda x: x[0]).toDF()\r\n",
        "\r\n",
        "    # Rename the dataframe columns as the first row in current dataframe after deleting 8 lines\r\n",
        "    schema_from_first_row = StructType([ StructField(field_name, StringType(), True)  for field_name in df.first() ])\r\n",
        "    df = spark.createDataFrame(df.rdd, schema=schema_from_first_row)\r\n",
        "\r\n",
        "    # Drop the /COUNT/ columns for now\r\n",
        "    regex = re.compile(\".*COUNT.*\")\r\n",
        "    for col in list(filter(regex.match, df.schema.names)):\r\n",
        "      df = df.drop(col)\r\n",
        "    \r\n",
        "    # Add a FILE column for backtracking purposes\r\n",
        "    file_category = file_name.replace('.csv','')\r\n",
        "    df = df.withColumn('FILE',lit(file_category))\r\n",
        "    \r\n",
        "    # Create temp views in spark sql for later processing via SQL\r\n",
        "    df.createOrReplaceTempView(file_category)\r\n",
        "\r\n",
        "    print('\\nPre-formatted the input data for {} successfully!'.format(file_name))\r\n",
        "    return df\r\n",
        "  except Exception as e:\r\n",
        "    print('\\nSomething went wrong while pre-formatting the input data. Please check if the file is currently in use.')\r\n",
        "    print(e)"
      ],
      "outputs": [],
      "metadata": {
        "id": "K3uofw66kqSa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "anatomical_structure_dfs = {}\r\n",
        "# max_cols = float('-inf')\r\n",
        "# csv_with_max_cols = ''\r\n",
        "for csv in csvs:\r\n",
        "  anatomical_structure_dfs[csv] = preformat_union_csvs(csv)\r\n",
        "'''\r\n",
        "  if max_cols < len(anatomical_structure_dfs[csv].columns):\r\n",
        "    max_cols = len(anatomical_structure_dfs[csv].columns)\r\n",
        "    csv_with_max_cols = csv\r\n",
        "'''\r\n",
        "print(csv_with_max_cols,' : ',max_cols)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'csvs' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1216/963506139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# max_cols = float('-inf')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# csv_with_max_cols = ''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mcsv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsvs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m   \u001b[0manatomical_structure_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreformat_union_csvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m '''\n",
            "\u001b[1;31mNameError\u001b[0m: name 'csvs' is not defined"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6_YaOZPj9cE",
        "outputId": "5683439e-85ab-44c0-fdf1-7b71d5f3dea2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "anatomical_structure_dfs['pbmc.csv'] = spark.sql(\"\"\"\r\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1-2' LAYER_CONNECTION, FILE from pbmc\r\n",
        "              UNION\r\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2-3' LAYER_CONNECTION, FILE from pbmc\r\n",
        "          \"\"\")\r\n",
        "anatomical_structure_dfs['pbmc.csv'].show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "haLIcIglpxbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "anatomical_structure_dfs['lung.csv'] = spark.sql(\"\"\"\r\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1-2' LAYER_CONNECTION, FILE from lung\r\n",
        "          \"\"\")\r\n",
        "anatomical_structure_dfs['lung.csv'].show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "q0LjA_l1oUMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "anatomical_structure_dfs['pancreas.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`,  `AS/1` AS `AS/2`,  `AS/1/LABEL` as `AS/2/LABEL`, `AS/1/ID` as `AS/1/ID`, '1-2' LAYER_CONNECTION, FILE from pancreas\n",
        "          \"\"\")\n",
        "anatomical_structure_dfs['pancreas.csv'].show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "c-L3sxytoivp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "anatomical_structure_dfs['kidney.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1-2' LAYER_CONNECTION, FILE from kidney\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2-3' LAYER_CONNECTION, FILE from kidney\n",
        "          \"\"\")\n",
        "anatomical_structure_dfs['kidney.csv'].show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jgwvj9rTpS3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "anatomical_structure_dfs['motor_cortex.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1-2' LAYER_CONNECTION, FILE from motor_cortex\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2-3' LAYER_CONNECTION, FILE from motor_cortex\n",
        "              UNION\n",
        "              SELECT `AS/3`, `AS/3/LABEL`, `AS/3/ID`, `AS/4`, `AS/4/LABEL`, `AS/4/ID`, '3-4' LAYER_CONNECTION, FILE from motor_cortex\n",
        "          \"\"\")\n",
        "anatomical_structure_dfs['motor_cortex.csv'].show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "00HCTV9Mjfr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Preprocessing hardcoded for now, should be dynamic based on varying number of column-headers"
      ],
      "metadata": {
        "id": "KfSBhTSBvOd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Manipulate each of the pyspark-sql views to bring them in the Source->Target network structure\n",
        "anatomical_structure_dfs['pbmc.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1->2' LAYER_CONNECTION, FILE from pbmc\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2->3' LAYER_CONNECTION, FILE from pbmc\n",
        "          \"\"\")\n",
        "\n",
        "anatomical_structure_dfs['lung.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1->2' LAYER_CONNECTION, FILE from lung\n",
        "          \"\"\")\n",
        "\n",
        "anatomical_structure_dfs['pancreas.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`,  `AS/1` AS `AS/2`,  `AS/1/LABEL` as `AS/2/LABEL`, `AS/1/ID` as `AS/1/ID`, '1->2' LAYER_CONNECTION, FILE from pancreas\n",
        "          \"\"\")\n",
        "\n",
        "anatomical_structure_dfs['kidney.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1->2' LAYER_CONNECTION, FILE from kidney\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2->3' LAYER_CONNECTION, FILE from kidney\n",
        "          \"\"\")\n",
        "\n",
        "anatomical_structure_dfs['motor_cortex.csv'] = spark.sql(\"\"\"\n",
        "              SELECT `AS/1`, `AS/1/LABEL`, `AS/1/ID`, `AS/2`, `AS/2/LABEL`, `AS/2/ID`, '1->2' LAYER_CONNECTION, FILE from motor_cortex\n",
        "              UNION\n",
        "              SELECT `AS/2`, `AS/2/LABEL`, `AS/2/ID`, `AS/3`, `AS/3/LABEL`, `AS/3/ID`, '2->3' LAYER_CONNECTION, FILE from motor_cortex\n",
        "              UNION\n",
        "              SELECT `AS/3`, `AS/3/LABEL`, `AS/3/ID`, `AS/4`, `AS/4/LABEL`, `AS/4/ID`, '3->4' LAYER_CONNECTION, FILE from motor_cortex\n",
        "          \"\"\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "o4ahwkPxgCjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "[(k, v.schema.names) for k,v in anatomical_structure_dfs.items()]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('lung.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/2/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE']),\n",
              " ('pancreas.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE']),\n",
              " ('kidney.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/2/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE']),\n",
              " ('pbmc.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/2/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE']),\n",
              " ('motor_cortex.csv',\n",
              "  ['AS/1',\n",
              "   'AS/1/LABEL',\n",
              "   'AS/1/ID',\n",
              "   'AS/2',\n",
              "   'AS/2/LABEL',\n",
              "   'AS/2/ID',\n",
              "   'LAYER_CONNECTION',\n",
              "   'FILE'])]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP28JlcMvgkd",
        "outputId": "35e4ada9-3db1-4bf1-e2b0-3a3eda304462"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Was planning on making the SQL statements more dynamic to handle varying number of column-names.\n",
        "\n",
        "#### For now have hardcoded the logic for each dataframe Union/Merge"
      ],
      "metadata": {
        "id": "rtLYFFXPuqWK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create a dummy target dataframe to store a Source->Target network structure\r\n",
        "tgt_schema = StructType([\r\n",
        "                         StructField('AS/1',StringType(),True), StructField('AS/1/LABEL',StringType(),True), \r\n",
        "                         StructField('AS/1/ID',StringType(),True), StructField('AS/2',StringType(),True), \r\n",
        "                         StructField('AS/2/LABEL',StringType(),True), StructField('AS/2/ID',StringType(),True), \r\n",
        "                         StructField('LAYER_CONNECTION',StringType(),True), StructField('FILE',StringType(),True)\r\n",
        "                         ])\r\n",
        "tgt_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema = tgt_schema)\r\n",
        "\r\n",
        "\r\n",
        "# For each standardized dataset, union it with the dummy tgt dataframe we created\r\n",
        "for csv in csvs:\r\n",
        "  tgt_df = tgt_df.union(anatomical_structure_dfs[csv])\r\n",
        "  '''\r\n",
        "  if len(anatomical_structure_dfs[csv].schema.names) > 6:\r\n",
        "    anatomical_structure_dfs[csv] = make_pivot_unions(anatomical_structure_dfs[csv])\r\n",
        "  else:\r\n",
        "    anatomical_structure_dfs[csv] = impute_values(anatomical_structure_dfs[csv])\r\n",
        "  '''\r\n",
        "\r\n",
        "tgt_df.schema.names = ['LABEL', 'EXPANDED_LABEL', 'AS/1/ID', 'LABEL', 'EXPANDED_LABEL', 'AS/2/ID', 'LAYER_CONNECTION', 'FILE']\r\n",
        "tgt_df.summary().show()\r\n",
        "\r\n",
        "tgt_df.toPandas().to_csv('New_Consolidated_Network_Python.csv', index=False, sep='\\t')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+-----------------+--------------------+-------+----------+--------------------+----------------+------+\n",
            "|summary|          AS/1|       AS/1/LABEL|             AS/1/ID|   AS/2|AS/2/LABEL|             AS/2/ID|LAYER_CONNECTION|  FILE|\n",
            "+-------+--------------+-----------------+--------------------+-------+----------+--------------------+----------------+------+\n",
            "|  count|           517|              517|                 515|    515|       515|                 513|             518|   518|\n",
            "|   mean|          null|           2146.0|                null|   null|      null|                null|            null|  null|\n",
            "| stddev|          null|538.8153672641492|                null|   null|      null|                null|            null|  null|\n",
            "|    min|\",\"CL:1001107\"|             1765|             AS/1/ID|   AS/1|AS/1/LABEL|             AS/1/ID|            1->2|kidney|\n",
            "|    25%|          null|           1765.0|                null|   null|      null|                null|            null|  null|\n",
            "|    50%|          null|           1765.0|                null|   null|      null|                null|            null|  null|\n",
            "|    75%|          null|           2527.0|                null|   null|      null|                null|            null|  null|\n",
            "|    max|       schwann|             vein|[kidney loop of H...|schwann|      vein|[kidney loop of H...|            3->4|  pbmc|\n",
            "+-------+--------------+-----------------+--------------------+-------+----------+--------------------+----------------+------+\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "Sjp3VZhtM6HO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb55d7a-164f-49ee-a974-927923211fc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Was planning on creating a network with multiple layers in between, so would have been working with dataframes of different numbers of columns.\n",
        "```\n",
        "from pyspark.sql import SparkSession, HiveContext\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql import Row\n",
        "\n",
        "def custom_union(df1, df2):\n",
        "    df1_cols = df1.columns\n",
        "    df2_cols = df2.columns\n",
        "    total_cols = sorted(df1_cols + list(set(df2_cols) - set(df1_cols)))\n",
        "    def expr(mycols, allcols):\n",
        "        def process_cols(colname):\n",
        "            if colname in mycols:\n",
        "                return colname\n",
        "            else:\n",
        "                return lit(None).alias(colname)\n",
        "        cols = map(process_cols, allcols)\n",
        "        return list(cols)\n",
        "        \n",
        "    appended = df1.select(expr(df1_cols, total_cols)).union(df2.select(expr(df2_cols, total_cols)))\n",
        "    return appended\n",
        "\n",
        "\n",
        "df = custom_union(parent_df, anatomical_structure_dfs['pancreas.csv'])\n",
        "df = custom_union(df, anatomical_structure_dfs[''])\n",
        "```\n"
      ],
      "metadata": {
        "id": "dlmTDYdmsdxJ"
      }
    }
  ]
}